{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2faf12b5",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d37a11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01ec1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data_labels_mainData.csv into a DataFrame\n",
    "main_data = pd.read_csv('data_labels_mainData.csv')\n",
    "\n",
    "#Import data_labels_extraData.csv into a DataFrame\n",
    "extra_data = pd.read_csv('data_labels_extraData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e698586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_task, val_task = train_test_split(main_data[['ImageName','cellType']], \n",
    "                                              test_size=0.2, random_state=9)                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ff5736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def GetImage(directory):\n",
    "    images=[]\n",
    "    for name in tqdm(directory, desc=\"Adding images\"):\n",
    "        image = cv2.imread(\"patch_images/\"+name)\n",
    "        dim = (32,32)\n",
    "        image = cv2.resize(image, dim)\n",
    "        image = Image.fromarray(image,'RGB')\n",
    "        images.append(np.array(image))\n",
    "    result = np.array(images)\n",
    "    print(\"\\ngetImage COMPLETED!\")\n",
    "    return result\n",
    "\n",
    "#Create a function to generate sample to fix the Imblance of the dataset\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "def GenerateSample(X,Y):\n",
    "    ros = RandomOverSampler(random_state = 1)\n",
    "    x, y = ros.fit_resample(X.values.reshape(-1,1), Y)\n",
    "    x = x.flatten()\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1806e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding images: 100%|██████████| 13128/13128 [00:02<00:00, 5770.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getImage COMPLETED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding images: 100%|██████████| 1980/1980 [00:00<00:00, 5684.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getImage COMPLETED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = train_task['ImageName']\n",
    "y_train = train_task['cellType']\n",
    "\n",
    "#Generate sample\n",
    "x_train, y_train = GenerateSample(x_train,y_train)\n",
    "x_train = GetImage(x_train)\n",
    "\n",
    "x_test = val_task['ImageName']\n",
    "x_test = GetImage(x_test)\n",
    "\n",
    "y_test = val_task['cellType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d676088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (13128, 32, 32, 3) - y_train shape: (13128,)\n",
      "x_test shape: (1980, 32, 32, 3) - y_test shape: (1980,)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "\n",
    "# Display shapes of train and test datasets\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c293f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.02),\n",
    "        layers.RandomWidth(0.2),\n",
    "        layers.RandomHeight(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "377f5f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " sequential_5 (Sequential)   (None, 32, 32, 3)         7         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,564,807\n",
      "Trainable params: 23,519,360\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_encoder():\n",
    "    resnet = keras.applications.ResNet50V2(\n",
    "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
    "    )\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    outputs = resnet(augmented)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
    "    return model\n",
    "\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 265\n",
    "hidden_units = 512\n",
    "projection_units = 128\n",
    "num_epochs = 50\n",
    "dropout_rate = 0.5\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84b8cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cellType-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "106f8fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cellType-classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,615,947\n",
      "Trainable params: 24,570,500\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 228s 4s/step - loss: 1.2981 - sparse_categorical_accuracy: 0.4550\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 220s 4s/step - loss: 0.8741 - sparse_categorical_accuracy: 0.6409\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 227s 4s/step - loss: 0.7789 - sparse_categorical_accuracy: 0.6869\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 233s 5s/step - loss: 0.7100 - sparse_categorical_accuracy: 0.7214\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 230s 5s/step - loss: 0.6714 - sparse_categorical_accuracy: 0.7322\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 230s 5s/step - loss: 0.6319 - sparse_categorical_accuracy: 0.7561\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 227s 5s/step - loss: 0.6095 - sparse_categorical_accuracy: 0.7617\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 212s 4s/step - loss: 0.6138 - sparse_categorical_accuracy: 0.7640\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 229s 5s/step - loss: 0.5720 - sparse_categorical_accuracy: 0.7842\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 227s 4s/step - loss: 0.5343 - sparse_categorical_accuracy: 0.7944\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 225s 4s/step - loss: 0.4978 - sparse_categorical_accuracy: 0.8147\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 200s 4s/step - loss: 0.4706 - sparse_categorical_accuracy: 0.8242\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 211s 4s/step - loss: 0.4532 - sparse_categorical_accuracy: 0.8315\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 203s 4s/step - loss: 0.4212 - sparse_categorical_accuracy: 0.8449\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 225s 4s/step - loss: 0.4292 - sparse_categorical_accuracy: 0.8426\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 196s 4s/step - loss: 0.4043 - sparse_categorical_accuracy: 0.8548\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 202s 4s/step - loss: 0.3720 - sparse_categorical_accuracy: 0.8648\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 214s 4s/step - loss: 0.3665 - sparse_categorical_accuracy: 0.8660\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 206s 4s/step - loss: 0.3800 - sparse_categorical_accuracy: 0.8640\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 211s 4s/step - loss: 0.3375 - sparse_categorical_accuracy: 0.8794\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 219s 4s/step - loss: 0.3230 - sparse_categorical_accuracy: 0.8845\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 216s 4s/step - loss: 0.3023 - sparse_categorical_accuracy: 0.8937\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 224s 4s/step - loss: 0.3259 - sparse_categorical_accuracy: 0.8849\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 209s 4s/step - loss: 0.2908 - sparse_categorical_accuracy: 0.8956\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 225s 4s/step - loss: 0.2612 - sparse_categorical_accuracy: 0.9100\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 210s 4s/step - loss: 0.2636 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 219s 4s/step - loss: 0.2509 - sparse_categorical_accuracy: 0.9134\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 207s 4s/step - loss: 0.2517 - sparse_categorical_accuracy: 0.9143\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 221s 4s/step - loss: 0.4619 - sparse_categorical_accuracy: 0.8374\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 211s 4s/step - loss: 0.3576 - sparse_categorical_accuracy: 0.8703\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 236s 5s/step - loss: 0.3796 - sparse_categorical_accuracy: 0.8612\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 218s 4s/step - loss: 0.2993 - sparse_categorical_accuracy: 0.8921\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 227s 5s/step - loss: 0.2594 - sparse_categorical_accuracy: 0.9090\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 219s 4s/step - loss: 0.2373 - sparse_categorical_accuracy: 0.9158\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 237s 5s/step - loss: 0.2614 - sparse_categorical_accuracy: 0.9115\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 212s 4s/step - loss: 0.3001 - sparse_categorical_accuracy: 0.8933\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 203s 4s/step - loss: 0.2465 - sparse_categorical_accuracy: 0.9143\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 222s 4s/step - loss: 0.1965 - sparse_categorical_accuracy: 0.9339\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 231s 5s/step - loss: 0.1885 - sparse_categorical_accuracy: 0.9346\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 216s 4s/step - loss: 0.1802 - sparse_categorical_accuracy: 0.9380\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 227s 5s/step - loss: 0.1627 - sparse_categorical_accuracy: 0.9409\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 211s 4s/step - loss: 0.1548 - sparse_categorical_accuracy: 0.9446\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 253s 5s/step - loss: 0.1530 - sparse_categorical_accuracy: 0.9467\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 226s 5s/step - loss: 0.1460 - sparse_categorical_accuracy: 0.9503\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 231s 5s/step - loss: 0.1342 - sparse_categorical_accuracy: 0.9528\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 223s 4s/step - loss: 0.1457 - sparse_categorical_accuracy: 0.9487\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 228s 5s/step - loss: 0.1390 - sparse_categorical_accuracy: 0.9532\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 226s 5s/step - loss: 0.1307 - sparse_categorical_accuracy: 0.9552\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 224s 4s/step - loss: 0.1212 - sparse_categorical_accuracy: 0.9586\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 220s 4s/step - loss: 0.1337 - sparse_categorical_accuracy: 0.9557\n",
      "62/62 [==============================] - 4s 54ms/step - loss: 1.2445 - sparse_categorical_accuracy: 0.7864\n",
      "Test accuracy: 78.64%\n"
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "classifier = create_classifier(encoder)\n",
    "classifier.summary()\n",
    "\n",
    "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = create_encoder()\n",
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "history = encoder_with_projection_head.fit(\n",
    "    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
