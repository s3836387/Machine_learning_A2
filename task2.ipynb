{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69b336b",
   "metadata": {},
   "source": [
    "<hr style=\"border:0.1px solid gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f629d5c",
   "metadata": {},
   "source": [
    "# <center style='color:Red'> COSC2753 - Machine Learning </center> \n",
    "## <center style='color:Red'> Assignment 2 - Classify Images of Colon Cancer </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39870a75",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#-COSC2753---Machine-Learning-\" data-toc-modified-id=\"-COSC2753---Machine-Learning--1\"><center style=\"color:Red\"> COSC2753 - Machine Learning </center></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Assignment-2---Classify-Images-of-Colon-Cancer-\" data-toc-modified-id=\"-Assignment-2---Classify-Images-of-Colon-Cancer--1.1\"><center style=\"color:Red\"> Assignment 2 - Classify Images of Colon Cancer </center></a></span></li></ul></li><li><span><a href=\"#Main-Task\" data-toc-modified-id=\"Main-Task-2\">Main Task</a></span></li><li><span><a href=\"#1.-Import-libraries-and-setting-up-environment\" data-toc-modified-id=\"1.-Import-libraries-and-setting-up-environment-3\">1. Import libraries and setting up environment</a></span></li><li><span><a href=\"#2.-Import-Dataset\" data-toc-modified-id=\"2.-Import-Dataset-4\">2. Import Dataset</a></span></li><li><span><a href=\"#3.-Exploratory-Data-Analysis-(EDA)\" data-toc-modified-id=\"3.-Exploratory-Data-Analysis-(EDA)-5\">3. Exploratory Data Analysis (EDA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1.-Process-the-data\" data-toc-modified-id=\"3.1.-Process-the-data-5.1\">3.1. Process the data</a></span></li><li><span><a href=\"#3.2.-Data-Analysis-and-Visualiztion\" data-toc-modified-id=\"3.2.-Data-Analysis-and-Visualiztion-5.2\">3.2. Data Analysis and Visualiztion</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.2.1.-Correlation-and-Correlation-Matrix\" data-toc-modified-id=\"3.2.1.-Correlation-and-Correlation-Matrix-5.2.1\">3.2.1. Correlation and Correlation Matrix</a></span></li><li><span><a href=\"#3.2.2.-Data-Distribution\" data-toc-modified-id=\"3.2.2.-Data-Distribution-5.2.2\">3.2.2. Data Distribution</a></span></li><li><span><a href=\"#Countplot\" data-toc-modified-id=\"Countplot-5.2.3\">Countplot</a></span></li><li><span><a href=\"#Histogram\" data-toc-modified-id=\"Histogram-5.2.4\">Histogram</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5.2.5\">Conclusion</a></span></li></ul></li><li><span><a href=\"#3.3.-Data-Preparation\" data-toc-modified-id=\"3.3.-Data-Preparation-5.3\">3.3. Data Preparation</a></span></li></ul></li><li><span><a href=\"#4.-Models\" data-toc-modified-id=\"4.-Models-6\">4. Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Convolutional-neural-network\" data-toc-modified-id=\"4.1-Convolutional-neural-network-6.1\">4.1 Convolutional neural network</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1.1-Baseline-model\" data-toc-modified-id=\"4.1.1-Baseline-model-6.1.1\">4.1.1 Baseline model</a></span></li><li><span><a href=\"#4.1.2-Hyperparameter-tuning\" data-toc-modified-id=\"4.1.2-Hyperparameter-tuning-6.1.2\">4.1.2 Hyperparameter tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#i.-Layers-tuning\" data-toc-modified-id=\"i.-Layers-tuning-6.1.2.1\">i. Layers tuning</a></span></li><li><span><a href=\"#With-drop-out-layer\" data-toc-modified-id=\"With-drop-out-layer-6.1.2.2\">With drop out layer</a></span></li><li><span><a href=\"#32-64-128-Dense:512-No-Dropout\" data-toc-modified-id=\"32-64-128-Dense:512-No-Dropout-6.1.2.3\">32-64-128 Dense:512 No Dropout</a></span></li></ul></li><li><span><a href=\"#4.1.3-Final-model\" data-toc-modified-id=\"4.1.3-Final-model-6.1.3\">4.1.3 Final model</a></span></li></ul></li><li><span><a href=\"#Fine-Tune\" data-toc-modified-id=\"Fine-Tune-6.2\">Fine Tune</a></span></li></ul></li><li><span><a href=\"#Function-to-create-model,-required-for-KerasClassifier\" data-toc-modified-id=\"Function-to-create-model,-required-for-KerasClassifier-7\">Function to create model, required for KerasClassifier</a></span></li><li><span><a href=\"#fix-random-seed-for-reproducibility\" data-toc-modified-id=\"fix-random-seed-for-reproducibility-8\">fix random seed for reproducibility</a></span></li><li><span><a href=\"#define-the-grid-search-parameters\" data-toc-modified-id=\"define-the-grid-search-parameters-9\">define the grid search parameters</a></span></li><li><span><a href=\"#summarize-results\" data-toc-modified-id=\"summarize-results-10\">summarize results</a></span></li><li><span><a href=\"#summarize-history-for-accuracy\" data-toc-modified-id=\"summarize-history-for-accuracy-11\">summarize history for accuracy</a></span></li><li><span><a href=\"#summarize-history-for-loss\" data-toc-modified-id=\"summarize-history-for-loss-12\">summarize history for loss</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa65e3",
   "metadata": {},
   "source": [
    "**Group Number:** 8\n",
    "\n",
    "**Student 1:** Ngo Quang Khai - s3836387 \n",
    "\n",
    "**Student 2:** Nguyen Quoc Minh - s3758994\n",
    "\n",
    "**Student 3:** Bui Thanh Huy - s3740934\n",
    "\n",
    "**Student 4:** Tran Vinh Khang - s3855823\n",
    "\n",
    "**Student 5:** Le Trong Hung - s3805504\n",
    "\n",
    "**Lecturer:** Dr. Bao Nguyen\n",
    "\n",
    "**Due Date:** May 14th, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7d6af",
   "metadata": {},
   "source": [
    "<hr style=\"border:0.1px solid gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958fcfd",
   "metadata": {},
   "source": [
    "# Main Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b10ae1e",
   "metadata": {},
   "source": [
    "- **Task 1**: Classify images according to whether given cell image represents a cancerous cells or not (isCancerous)\n",
    "\n",
    "\n",
    "- **Task 2**: Classify images according to cell-type, such as: fibroblast, inflammatory, epithelial or others.\n",
    "\n",
    "\n",
    "- **Extra task**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc28c3",
   "metadata": {},
   "source": [
    "# 1. Import libraries and setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras\n",
    "# !pip install opencv-python\n",
    "# !pip install bayesian-optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45899c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score, classification_report\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold,GridSearchCV\n",
    "\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is important to check the version of these libraries to make sure that implement the latest method.\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('CV2 version:', cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c253d70",
   "metadata": {},
   "source": [
    "# 2. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ee400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data_labels_mainData.csv into a DataFrame\n",
    "main_data = pd.read_csv('data_labels_mainData.csv')\n",
    "\n",
    "# Import data_labels_extraData.csv into a DataFrame\n",
    "extra_data = pd.read_csv('data_labels_extraData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d64513",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b23ab",
   "metadata": {},
   "source": [
    "## 3.1. Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7620550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the data frame of main_data\n",
    "print(\"Dataframe of main_data: \\n\", main_data, \"\\n\")\n",
    "\n",
    "#See the data frame of extra_data\n",
    "print(\"Dataframe of extra_data: \\n\", extra_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca9884",
   "metadata": {},
   "source": [
    "**=>** Next, we will check if the data have any null values and datatype of each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.info()\n",
    "main_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7dc0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_data.info()\n",
    "extra_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553755d",
   "metadata": {},
   "source": [
    "**=>** Since our data is no missing/null value and properly formatted, we will move on to analyzing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ff5d3",
   "metadata": {},
   "source": [
    "## 3.2. Data Analysis and Visualiztion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffbe57",
   "metadata": {},
   "source": [
    "Based on the information provided by the biomedical company as well as the DataFrames in **section 3.1** above. We are aware of:\n",
    "- The **main_data** contains both **cellType (0 to 3)** and **isCancerous (0 and 1)** columns. In there, **cellType** values 0 to 3 represent fibroblast, inflammatory, epithelial, and miscellaneous cell types respectively based on the **cellTypeName** column. In addition, **isCancerous** values 0 and 1 represent colon cells with non-Cancer and Cancer.\n",
    "\n",
    "\n",
    "- The **extra_data** only contains **isCancerous (0 and 1)** column.\n",
    "\n",
    "**=>** Therefore, we choose the **isCancerous** attribute as the target column because both **main_data** and **extra_data** have the column **isCancerous**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecfa939",
   "metadata": {},
   "source": [
    "### 3.2.1. Correlation and Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e95090",
   "metadata": {},
   "source": [
    "Firstly, we will draw the correlation matrix to find out any relationship between **isCancerous** column with the others\n",
    "columns in **main_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd101a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(14, 10))\n",
    "plt.title('Correlation Matrix', fontsize=16);\n",
    "sns.heatmap(main_data.corr(), annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690da07",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "As we can see from the matrix above, most of the attributes do not correlate with **isCancerous** much are **InstanceID** (-0.44), **patientID** (-0.0012). Since these attributes do not affect much to isCancerous attribute, we will not use these attributes to train our model. Applying these two attributes to train the model can cause noise and overcomplicated issue to our prediction model.\n",
    "\n",
    "**=>** There is only attribute **cellType** with 0.44 has high impact on the attribute 'isCancerous'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfa007",
   "metadata": {},
   "source": [
    "### 3.2.2. Data Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f6a62",
   "metadata": {},
   "source": [
    "### Countplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the countplot to see the classification of cellType and isCancerous \n",
    "fig, ax =plt.subplots(1,2, figsize = (12,6))\n",
    "\n",
    "# 0 = fibroblast, 1 = imflammatory, 2 = epithelial, 3 = others\n",
    "sns.countplot(main_data['cellType'], palette='Set2', ax=ax[0]).set_title(\"Classification of Cell Types\")\n",
    "\n",
    "sns.countplot(main_data['isCancerous'], palette='Set1', ax=ax[1]).set_title(\"Classification of Cells isCancerous and Non-Cancerous\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647541d8",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "- The first countplot shows us there 4 different types of cell: 0, 1, 2, 3. The first type of cell is 0 has approximately 1900, the second type of cell is 2 has approximately 2600, the third one is 4100 and last one is 1500.\n",
    "\n",
    "\n",
    "- The second countplot shows us number of cells that are cancer and not. In the first column is 0 represent the cell that are not cancerous, and 1 represent the cell that are cancerous. There are approximately 5900 cells that are not cancerous and 4200 cells that are cancerous in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b791a84",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6360f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the histogram for cellType respected to isCancerous (class 0 and 1)\n",
    "g = sns.FacetGrid(main_data, col='isCancerous', height=4, aspect=2)\n",
    "g.map(plt.hist, str('cellType'), bins=20)\n",
    "g.set_axis_labels('cellType', \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f79792",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "Looking at these two histograms, we can see that 'cellType' 0, 1, and 3 are not cancerous. Meanwhile only 'cellType' 2 is cancerous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply loc to make sure\n",
    "main_data.loc[(main_data['cellType'] == 2) & (main_data['isCancerous'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd878ff",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- In **main_data**, only the attribute **cellType** continue to be our strongest candidate for to influence our model when it has high impact on our target attribute is **isCancerous**.\n",
    "\n",
    "\n",
    "- Attributes such as **InstanceID** and **patientID** might create a lot of noise in our model. \n",
    "\n",
    "\n",
    "- There are 4 types of cells: 0, 1, 2, 3. Only cell type number 2 is cancerous while others are not cancerous.\n",
    "\n",
    "\n",
    "- We expect the the model would predict class 0 more accurately than class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43f450",
   "metadata": {},
   "source": [
    "## 3.3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac75d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into train and validation \n",
    "train_task1, val_task1 = train_test_split(main_data[['ImageName', 'isCancerous']], \n",
    "                                              test_size=0.3, random_state=9)\n",
    "\n",
    "\n",
    "train_task2, val_task2 = train_test_split(main_data[['ImageName','cellType']], \n",
    "                                              test_size=0.3, random_state=9)                                        \n",
    "\n",
    "print('Training data shape task 1:', train_task1.shape)\n",
    "print('Validation data shape task 1:', val_task1.shape)\n",
    "\n",
    "print('Training data shape task 2:', train_task2.shape)\n",
    "print('Validation data shape task 2:', val_task2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a a function to add image according to the name given from the list\n",
    "\n",
    "def GetImage(directory):\n",
    "    images=[]\n",
    "    for name in tqdm(directory, desc=\"Adding images\"):\n",
    "        image = cv2.imread(\"patch_images/\"+name)\n",
    "        image = Image.fromarray(image,'RGB')\n",
    "        images.append(np.array(image))\n",
    "    result = np.array(images)\n",
    "    print(\"\\ngetImage COMPLETED!\")\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to generate sample to fix the Imblance of the dataset\n",
    "def GenerateSample(X,Y):\n",
    "    ros = RandomOverSampler(random_state = 1)\n",
    "    x, y = ros.fit_resample(X.values.reshape(-1,1), Y)\n",
    "    x = x.flatten()\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train = train_task1['ImageName']\n",
    "y1_train = train_task1['isCancerous']\n",
    "print(\"Original Dataset:\\n\",y1_train.value_counts())\n",
    "\n",
    "#Generate sample\n",
    "x1_train, y1_train = GenerateSample(x1_train,y1_train)\n",
    "print(\"Sampled Dataset:\\n\",y1_train.value_counts())\n",
    "x1_train = GetImage(x1_train)\n",
    "\n",
    "x1_test = val_task1['ImageName']\n",
    "x1_test = GetImage(x1_test)\n",
    "\n",
    "y1_test = val_task1['isCancerous']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_train = train_task2['ImageName']\n",
    "y2_train = train_task2['cellType']\n",
    "print(\"Original Dataset:\\n\",y2_train.value_counts())\n",
    "\n",
    "#Generate sample\n",
    "x2_train, y2_train = GenerateSample(x2_train,y2_train)\n",
    "print(\"Sampled Dataset:\\n\",y2_train.value_counts())\n",
    "x2_train = GetImage(x2_train)\n",
    "\n",
    "x2_test = val_task2['ImageName']\n",
    "x2_test = GetImage(x2_test)\n",
    "\n",
    "y2_test = val_task2['cellType']\n",
    "\n",
    "x2_test,x2_val,y2_test,y2_val = train_test_split(x2_test, y2_test,\n",
    "                                              test_size=0.5, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fa11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TASK 1 SHAPE:\")\n",
    "print(\"TRAIN SHAPE:\")\n",
    "print(\"x1 shape:\", x1_train.shape)\n",
    "print(\"y1 shape:\", y1_train.shape)\n",
    "print(\"TEST SHAPE:\")\n",
    "print(\"x1 shape:\", x1_test.shape)\n",
    "print(\"y1 shape:\", y1_test.shape)\n",
    "\n",
    "print(\"TASK 2 SHAPE:\")\n",
    "print(\"x1 shape:\", x2_train.shape)\n",
    "print(\"y1 shape:\", y2_train.shape)\n",
    "print(\"VALIDATION SHAPE:\")\n",
    "print(\"x1 shape:\", x2_val.shape)\n",
    "print(\"y1 shape:\", y2_val.shape)\n",
    "print(\"TEST SHAPE:\")\n",
    "print(\"x1 shape:\", x2_test.shape)\n",
    "print(\"y1 shape:\", y2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train_scaled = x2_train / 255\n",
    "X2_val_scaled = x2_val / 255\n",
    "X2_test_scaled = x2_test / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc56c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\",input_shape=(27,27,3)),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d363c85",
   "metadata": {},
   "source": [
    "# 4. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cce0ad",
   "metadata": {},
   "source": [
    "## 4.1 Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8aa6ab",
   "metadata": {},
   "source": [
    "### 4.1.1 Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b17dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_baseline = Sequential([ \n",
    "    data_augmentation, \n",
    "    layers.Conv2D(16, (3, 3), activation='relu', padding='same'), \n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, kernel_regularizer=l2(l=0.01), activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_baseline.compile(optimizer=\"adam\" , \n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                      metrics=['accuracy']) \n",
    "with tf.device('/GPU:0'): \n",
    "    cnn_baseline.fit(X2_train_scaled,y2_train, epochs=50,verbose=1)\n",
    "\n",
    "predict=cnn_baseline.predict(X2_val_scaled) classes=np.argmax(predict,axis=1)\n",
    "\n",
    "print(classification_report(y2_val, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b969744",
   "metadata": {},
   "source": [
    "### 4.1.2 Hyperparameter tuning\n",
    "#### i. Layers tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab03817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1e6170e",
   "metadata": {},
   "source": [
    "#### With drop out layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50ac89",
   "metadata": {},
   "source": [
    "#### 32-64-128 Dense:512 No Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f83b94",
   "metadata": {},
   "source": [
    "model_no_drop = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, kernel_regularizer=l2(l=0.01), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(512, kernel_regularizer=l2(l=0.01), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15538ea",
   "metadata": {},
   "source": [
    "\n",
    "model_no_drop.compile(optimizer=\"adam\" , \n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                      metrics=['accuracy'])\n",
    "with tf.device('/GPU:0'): \n",
    "    model_no_drop.fit(X2_train_scaled,y2_train, epochs=30 ,verbose=1) \n",
    "\n",
    "predict=model_no_drop.predict(X2_test_scaled)\n",
    "classes=np.argmax(predict,axis=1)\n",
    "\n",
    "print(classification_report(y2_test, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74cd95",
   "metadata": {},
   "source": [
    "### 4.1.3 Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e6130",
   "metadata": {},
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18e066",
   "metadata": {},
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Dropout(0.4),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, kernel_regularizer=l2(l=0.01), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, kernel_regularizer=l2(l=0.01), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e59332",
   "metadata": {},
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "opt = Adam(lr=0.0005, beta_1=0.4, beta_2=0.444, decay=1.0e-6, amsgrad=True)\n",
    "model.compile(optimizer=opt , loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8eae67",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.001, patience=10, verbose=1, mode='auto'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"task2/task2_weights.{epoch:02d}-valloss.{val_loss:.2f}.h5\",\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=6, min_lr=0.5e-15) \n",
    "]\n",
    "\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/GPU:0'): \n",
    "    history = model.fit(X2_train_scaled, y2_train,\n",
    "                    validation_data=(X2_val_scaled, y2_val), \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=EPOCHS, \n",
    "                    verbose=1,\n",
    "                    callbacks=my_callbacks)\n",
    "\n",
    "print(\"---  Training time in seconds ---%s \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b211fe45",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scorer accuracy\n",
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308617df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    def nn_cl_fun():\n",
    "        opt = Adam(lr = learning_rate)\n",
    "        nn = Sequential([\n",
    "        data_augmentation,\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(), \n",
    "        layers.MaxPooling2D(2,2), \n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2,2),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2,2),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, kernel_regularizer=l2(l=0.01), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(512, kernel_regularizer=l2(l=0.01), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(4, activation='softmax')\n",
    "        ])\n",
    "        nn.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=opt, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X2_train_scaled, y2_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nn ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683eadc7",
   "metadata": {},
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential([\n",
    "        data_augmentation,\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(), \n",
    "        layers.MaxPooling2D(2,2), \n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2,2),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2,2),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, kernel_regularizer=l2(l=0.01), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(512, kernel_regularizer=l2(l=0.01), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    # Compile model\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X2_train_scaled,y2_train)\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460916a",
   "metadata": {},
   "source": [
    " model.load_weights(\"task2_weights.111-0.76.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c50cf1",
   "metadata": {},
   "source": [
    "\n",
    "scores = model.evaluate(X2_val_scaled, y2_val, verbose=1)\n",
    "print('Validation loss:', scores[0])\n",
    "print('Validation accuracy:', scores[1])\n",
    "print('Max Validation accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93baac1e",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=model.predict(X2_test_scaled)\n",
    "classes=np.argmax(predict,axis=1)\n",
    "\n",
    "print(classification_report(y2_test, classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"fibroblast\",\"inflammatory\",\"epithelial\",\"others\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783414b",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "batch_predict = model.predict (X2_test_scaled)\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i+1)\n",
    "    first_image = X2_test_scaled[i]\n",
    "    first_label = int(y2_test.iloc[i])\n",
    "    plt.imshow(first_image)\n",
    "    confidence = round(100 * (np.max(batch_predict[i][0])), 2)\n",
    "    actual_class= class_names[first_label]\n",
    "    predicted_class = class_names[np.argmax(batch_predict[i])]\n",
    "    \n",
    "    plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "    plt.axis(\"off\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
