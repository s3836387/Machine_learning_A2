{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4439c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn as sk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c4d7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data_labels_mainData.csv into a DataFrame\n",
    "main_data = pd.read_csv('data_labels_mainData.csv')\n",
    "\n",
    "# Import data_labels_extraData.csv into a DataFrame\n",
    "extra_data = pd.read_csv('data_labels_extraData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89798081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape task 1: (6927, 2)\n",
      "Validation data shape task 1: (2969, 2)\n",
      "Training data shape task 2: (6927, 2)\n",
      "Validation data shape task 2: (2969, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#splitting data into train and validation \n",
    "train_task1, val_task1 = train_test_split(main_data[['ImageName', 'isCancerous']], \n",
    "                                              test_size=0.3, random_state=9)\n",
    "\n",
    "\n",
    "train_task2, val_task2 = train_test_split(main_data[['ImageName','cellType']], \n",
    "                                              test_size=0.3, random_state=9)                                        \n",
    "\n",
    "print('Training data shape task 1:', train_task1.shape)\n",
    "print('Validation data shape task 1:', val_task1.shape)\n",
    "\n",
    "print('Training data shape task 2:', train_task2.shape)\n",
    "print('Validation data shape task 2:', val_task2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8208b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a a function to add image according to the name given from the list\n",
    "from PIL import Image\n",
    "def GetImage(directory):\n",
    "    images=[]\n",
    "    for name in tqdm(directory, desc=\"Adding images\"):\n",
    "        image = cv2.imread(\"patch_images/\"+name)\n",
    "        image = Image.fromarray(image,'RGB')\n",
    "        images.append(np.array(image))\n",
    "    result = np.array(images)\n",
    "    print(\"\\ngetImage COMPLETED!\")\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8feb245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to generate sample to fix the Imblance of the dataset\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "def GenerateSample(X,Y):\n",
    "    ros = RandomOverSampler(random_state = 1)\n",
    "    x, y = ros.fit_resample(X.values.reshape(-1,1), Y)\n",
    "    x = x.flatten()\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "796b7806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      " 0    4030\n",
      "1    2897\n",
      "Name: isCancerous, dtype: int64\n",
      "Sampled Dataset:\n",
      " 0    4030\n",
      "1    4030\n",
      "Name: isCancerous, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding images: 100%|██████████| 8060/8060 [00:19<00:00, 414.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getImage COMPLETED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding images: 100%|██████████| 2969/2969 [00:08<00:00, 369.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getImage COMPLETED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x1_train = train_task1['ImageName']\n",
    "y1_train = train_task1['isCancerous']\n",
    "print(\"Original Dataset:\\n\",y1_train.value_counts())\n",
    "\n",
    "#Generate sample\n",
    "x1_train, y1_train = GenerateSample(x1_train,y1_train)\n",
    "print(\"Sampled Dataset:\\n\",y1_train.value_counts())\n",
    "x1_train = GetImage(x1_train)\n",
    "\n",
    "x1_test = val_task1['ImageName']\n",
    "x1_test = GetImage(x1_test)\n",
    "\n",
    "y1_test = val_task1['isCancerous']\n",
    "train_ds = x1_train\n",
    "val_ds = y1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd291563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      " 2    2897\n",
      "1    1778\n",
      "0    1302\n",
      "3     950\n",
      "Name: cellType, dtype: int64\n",
      "Sampled Dataset:\n",
      " 1    2897\n",
      "2    2897\n",
      "3    2897\n",
      "0    2897\n",
      "Name: cellType, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding images: 100%|██████████| 11588/11588 [00:01<00:00, 6063.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getImage COMPLETED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding images: 100%|██████████| 2969/2969 [00:00<00:00, 6090.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getImage COMPLETED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x2_train = train_task2['ImageName']\n",
    "y2_train = train_task2['cellType']\n",
    "print(\"Original Dataset:\\n\",y2_train.value_counts())\n",
    "\n",
    "#Generate sample\n",
    "x2_train, y2_train = GenerateSample(x2_train,y2_train)\n",
    "print(\"Sampled Dataset:\\n\",y2_train.value_counts())\n",
    "x2_train = GetImage(x2_train)\n",
    "\n",
    "x2_test = val_task2['ImageName']\n",
    "x2_test = GetImage(x2_test)\n",
    "\n",
    "y2_test = val_task2['cellType']\n",
    "\n",
    "x2_test,x2_val,y2_test,y2_val = train_test_split(x2_test, y2_test,\n",
    "                                              test_size=0.5, random_state=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d4f67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK 1 SHAPE:\n",
      "TRAIN SHAPE:\n",
      "x1 shape: (8060, 27, 27, 3)\n",
      "y1 shape: (8060,)\n",
      "TEST SHAPE:\n",
      "x1 shape: (2969, 27, 27, 3)\n",
      "y1 shape: (2969,)\n",
      "TASK 2 SHAPE:\n",
      "x1 shape: (11588, 27, 27, 3)\n",
      "y1 shape: (11588,)\n",
      "VALIDATION SHAPE:\n",
      "x1 shape: (1485, 27, 27, 3)\n",
      "y1 shape: (1485,)\n",
      "TEST SHAPE:\n",
      "x1 shape: (1484, 27, 27, 3)\n",
      "y1 shape: (1484,)\n"
     ]
    }
   ],
   "source": [
    "print(\"TASK 1 SHAPE:\")\n",
    "print(\"TRAIN SHAPE:\")\n",
    "print(\"x1 shape:\", x1_train.shape)\n",
    "print(\"y1 shape:\", y1_train.shape)\n",
    "print(\"TEST SHAPE:\")\n",
    "print(\"x1 shape:\", x1_test.shape)\n",
    "print(\"y1 shape:\", y1_test.shape)\n",
    "\n",
    "print(\"TASK 2 SHAPE:\")\n",
    "print(\"x1 shape:\", x2_train.shape)\n",
    "print(\"y1 shape:\", y2_train.shape)\n",
    "print(\"VALIDATION SHAPE:\")\n",
    "print(\"x1 shape:\", x2_val.shape)\n",
    "print(\"y1 shape:\", y2_val.shape)\n",
    "print(\"TEST SHAPE:\")\n",
    "print(\"x1 shape:\", x2_test.shape)\n",
    "print(\"y1 shape:\", y2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bae4a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size is: 3\n",
      "###### Create Training Data ######\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tfsim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size is: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(classes_per_batch, num_known_classes) \u001b[38;5;241m*\u001b[39m examples_per_class_per_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Create Training Data \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mcenter(\u001b[38;5;241m34\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 22\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtfsim\u001b[49m\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mSingleShotMemorySampler(\n\u001b[0;32m     23\u001b[0m     x1_train,\n\u001b[0;32m     24\u001b[0m     y1_train,\n\u001b[0;32m     25\u001b[0m     examples_per_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Create Validation Data \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mcenter(\u001b[38;5;241m34\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     30\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m tfsim\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTFDatasetMultiShotMemorySampler(\n\u001b[0;32m     31\u001b[0m     x1_test,\n\u001b[0;32m     32\u001b[0m     classes_per_batch\u001b[38;5;241m=\u001b[39mclasses_per_batch,\n\u001b[0;32m     33\u001b[0m     splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m     total_examples_per_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     35\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfsim' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_known_classes = 3\n",
    "class_list = random.sample(population=range(3), k=num_known_classes)\n",
    "\n",
    "classes_per_batch = 3\n",
    "# Passing multiple examples per class per batch ensures that each example has\n",
    "# multiple positive pairs. This can be useful when performing triplet mining or\n",
    "# when using losses like `MultiSimilarityLoss` or `CircleLoss` as these can\n",
    "# take a weighted mix of all the positive pairs. In general, more examples per\n",
    "# class will lead to more information for the positive pairs, while more classes\n",
    "# per batch will provide more varied information in the negative pairs. However,\n",
    "# the losses compute the pairwise distance between the examples in a batch so\n",
    "# the upper limit of the batch size is restricted by the memory.\n",
    "examples_per_class_per_batch = 1\n",
    "\n",
    "print(\n",
    "    \"Batch size is: \"\n",
    "    f\"{min(classes_per_batch, num_known_classes) * examples_per_class_per_batch}\"\n",
    ")\n",
    "\n",
    "print(\" Create Training Data \".center(34, \"#\"))\n",
    "train_ds = tfsim.samplers.SingleShotMemorySampler(\n",
    "    x1_train,\n",
    "    y1_train,\n",
    "    examples_per_batch = 2\n",
    "\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \" Create Validation Data \".center(34, \"#\"))\n",
    "val_ds = tfsim.samplers.TFDatasetMultiShotMemorySampler(\n",
    "    x1_test,\n",
    "    classes_per_batch=classes_per_batch,\n",
    "    splits=\"test\",\n",
    "    total_examples_per_class=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1ade7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_similarity as tfsim\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tfsim.utils.tf_cap_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afd73fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"similarity_model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " rescaling_2 (Rescaling)     (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 30, 30, 64)        1792      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 30, 30, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 28, 28, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 5, 5, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 5, 5, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 3, 3, 256)         590080    \n",
      "                                                                 \n",
      " global_max_pooling2d_2 (Glo  (None, 256)              0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " metric_embedding_2 (MetricE  (None, 256)              65792     \n",
      " mbedding)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,028,480\n",
      "Trainable params: 1,027,584\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 256\n",
    "\n",
    "inputs = keras.layers.Input((32, 32, 3))\n",
    "x = keras.layers.Rescaling(scale=1.0 / 255)(inputs)\n",
    "x = keras.layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(128, 3, activation=\"relu\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D((4, 4))(x)\n",
    "x = keras.layers.Conv2D(256, 3, activation=\"relu\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(256, 3, activation=\"relu\")(x)\n",
    "x = keras.layers.GlobalMaxPool2D()(x)\n",
    "outputs = tfsim.layers.MetricEmbedding(embedding_size)(x)\n",
    "\n",
    "# building model\n",
    "model = tfsim.models.SimilarityModel(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "377e7e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance metric automatically set to cosine use the distance arg to override.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# compiling and training\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate), loss\u001b[38;5;241m=\u001b[39mloss, steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_steps\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\an_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\an_env\\lib\\site-packages\\pandas\\core\\generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1530\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "learning_rate = 0.002\n",
    "val_steps = 50\n",
    "\n",
    "# init similarity loss\n",
    "loss = tfsim.losses.MultiSimilarityLoss()\n",
    "\n",
    "# compiling and training\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate), loss=loss, steps_per_execution=10,\n",
    ")\n",
    "history = model.fit(\n",
    "    train_ds, epochs=epochs, validation_data=val_ds, validation_steps=val_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e7a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
